import os
import time
import pandas as pd
from pymongo import MongoClient, UpdateOne
import sys
from datetime import datetime

def conversion(row):
    """Fonction de conversion des donn√©es"""
    return {
        "patient": {
            "Name": row["Name"],
            "Gender": row["Gender"],
            "Blood_Type": row["Blood_Type"],
            "Age": row["Age"],
            "Medical_Condition": row["Medical_Condition"],
            "Date_of_Admission": pd.to_datetime(row["Date_of_Admission"]),
            "Admission_Type": row["Admission_Type"],
            "Discharge_Date": pd.to_datetime(row["Discharge_Date"]),
            "Room_Number": row["Room_Number"],
            "Doctor": row["Doctor"],
            "Hospital": row["Hospital"],
            "Billing_Amount": row["Billing_Amount"],
            "Insurance_Provider": row["Insurance_Provider"],
            "Medication": row["Medication"],
            "Test_Results": row["Test_Results"]
        },
        "metadata": {
            "created_at": datetime.now(),
            "created_by": "migrator_service",
            "version": "1.0"
        }
    }

def migrate_in_batches(collection, cleaned_df, batch_size=1000):
    """Migration par lots - BEAUCOUP plus rapide"""
    total_migrated = 0
    total_rows = len(cleaned_df)
    
    print(f"üöÄ D√©but migration par lots de {batch_size} documents")
    start_time = time.time()
    
    for i in range(0, total_rows, batch_size):
        batch_start = time.time()
        batch = cleaned_df.iloc[i:i+batch_size]
        operations = []
        
        # Pr√©parer toutes les op√©rations du lot
        for _, row in batch.iterrows():
            query = {
                "patient.Name": row["Name"],
                "patient.Date_of_Admission": pd.to_datetime(row["Date_of_Admission"]),
                "patient.Doctor": row["Doctor"]
            }
            update = {"$set": conversion(row)}
            operations.append(UpdateOne(query, update, upsert=True))
        
        # Ex√©cuter TOUT le lot d'un coup
        try:
            if operations:
                result = collection.bulk_write(operations, ordered=False)
                batch_migrated = result.upserted_count + result.modified_count
                total_migrated += batch_migrated
                
                batch_time = time.time() - batch_start
                progress = (i + len(batch)) / total_rows * 100
                
                print(f"üìä Lot {i//batch_size + 1}: {i + len(batch)}/{total_rows} "
                      f"({progress:.1f}%) - {batch_migrated} docs - {batch_time:.2f}s")
                
        except Exception as e:
            print(f"‚ùå Erreur lot {i//batch_size + 1}: {e}")
            # Continuer avec le lot suivant
            continue
    
    total_time = time.time() - start_time
    print(f"‚úÖ Migration termin√©e: {total_migrated} docs en {total_time:.2f}s")
    return total_migrated

def wait_for_mongo(uri, max_retries=30):
    """Attendre MongoDB avec pool de connexions optimis√©"""
    print(f"Connexion √† MongoDB avec pool optimis√©...")
    for i in range(max_retries):
        try:
            # Configuration optimis√©e pour les gros volumes
            client = MongoClient(
                uri, 
                serverSelectionTimeoutMS=10000,
                connectTimeoutMS=20000,
                socketTimeoutMS=60000,  # 60 secondes pour les grosses op√©rations
                maxPoolSize=50,         # Plus de connexions simultan√©es
                minPoolSize=10,
                maxIdleTimeMS=30000,
                waitQueueTimeoutMS=10000
            )
            client.admin.command('ping')
            print("‚úÖ MongoDB connect√© avec pool optimis√© !")
            return client
        except Exception as e:
            print(f"‚è≥ Tentative {i+1}/{max_retries}: {e}")
            time.sleep(3)
    raise Exception("‚ùå Impossible de se connecter √† MongoDB")

def main():
    try:
        # Connexion optimis√©e
        mongo_uri = os.getenv('MONGO_URI', 'mongodb://user:pwuser@mongodb:27017/healthcare_db?authSource=healthcare_db')
        client = wait_for_mongo(mongo_uri)
        db = client['healthcare_db']
        collection = db['Patients']
        
        # Lecture et nettoyage
        print("üìÅ Lecture du fichier CSV...")
        df = pd.read_csv('data.csv')
        print(f"‚úÖ CSV lu: {len(df)} lignes")
        
        # Nettoyage (utilisez votre fonction existante)
        try:
            from cleaning import clean_df
            cleaned_df = clean_df(df)
        except ImportError:
            print("‚ö†Ô∏è Module cleaning non trouv√©, utilisation des donn√©es brutes")
            cleaned_df = df
        
        print(f"‚úÖ Donn√©es pr√©par√©es: {len(cleaned_df)} lignes")
        
        # Migration par lots (RAPIDE)
        migrated_count = migrate_in_batches(collection, cleaned_df, batch_size=1000)
        
        # V√©rification finale
        total_docs = collection.count_documents({})
        print(f"üìà Total final: {total_docs} documents")
        print(f"üéâ Migration r√©ussie: {migrated_count} documents migr√©s")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ERREUR: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        if 'client' in locals():
            client.close()

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
